{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Simple Linear Regression?\n",
        "\n",
        "- Simple Linear Regression is a method to model the relationship between two continuous variables using a straight line.\n",
        "\n",
        "Equation:\n",
        "\n",
        "Y\n",
        "=\n",
        "β\n",
        "0\n",
        "+\n",
        "β\n",
        "1\n",
        "X\n",
        "Y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X\n",
        "\n",
        "2. What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "- Key Assumptions of Simple Linear Regression:\n",
        "\n",
        "- Linearity: Relationship between X and Y is linear.\n",
        "- Independence: Observations are independent.\n",
        "- Homoscedasticity: Constant variance of errors.\n",
        "- Normality: Errors are normally distributed.\n",
        "- No Multicollinearity: (Not applicable here as there's only one predictor).\n",
        "\n",
        "3. What does the coefficient m represent in the equation Y=mX+c?\n",
        "\n",
        "- The coefficient m represents the slope of the line in the equation\n",
        "Y\n",
        "=\n",
        "m\n",
        "X\n",
        "+\n",
        "c\n",
        "Y=mX+c.\n",
        "\n",
        "It indicates the change in Y for a one-unit increase in X.\n",
        "\n",
        "4. What does the intercept c represent in the equation Y=mX+c?\n",
        "\n",
        "- The intercept c represents the value of Y when X = 0.\n",
        "\n",
        "It is the point where the line crosses the Y-axis.\n",
        "\n",
        "5. How do we calculate the slope m in Simple Linear Regression?\n",
        "\n",
        "- The slope\n",
        "m\n",
        "m is calculated using the formula:\n",
        "\n",
        "m\n",
        "=\n",
        "∑\n",
        "(\n",
        "X\n",
        "i\n",
        "−\n",
        "X\n",
        "ˉ\n",
        ")\n",
        "(\n",
        "Y\n",
        "i\n",
        "−\n",
        "Y\n",
        "ˉ\n",
        ")\n",
        "∑\n",
        "(\n",
        "X\n",
        "i\n",
        "−\n",
        "X\n",
        "ˉ\n",
        ")\n",
        "2\n",
        "m=\n",
        "∑(X\n",
        "i\n",
        "​\n",
        " −\n",
        "X\n",
        "ˉ\n",
        " )\n",
        "2\n",
        "\n",
        "∑(X\n",
        "i\n",
        "​\n",
        " −\n",
        "X\n",
        "ˉ\n",
        " )(Y\n",
        "i\n",
        "​\n",
        " −\n",
        "Y\n",
        "ˉ\n",
        " )\n",
        "​\n",
        "\n",
        "Where:\n",
        "\n",
        "X\n",
        "i\n",
        ",\n",
        "Y\n",
        "i\n",
        "X\n",
        "i\n",
        "​\n",
        " ,Y\n",
        "i\n",
        "​\n",
        "  are the data points\n",
        "X\n",
        "ˉ\n",
        ",\n",
        "Y\n",
        "ˉ\n",
        "X\n",
        "ˉ\n",
        " ,\n",
        "Y\n",
        "ˉ\n",
        "  are the means of X and Y respectively\n",
        "\n",
        "6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        "\n",
        "- The purpose of the least squares method is to minimize the sum of squared differences between the actual and predicted Y values.\n",
        "\n",
        "This helps find the best-fitting regression line.\n",
        "\n",
        "7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        "\n",
        "- The coefficient of determination (R²) measures how well the regression line explains the variability of the dependent variable.\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "R² = 1: Perfect fit\n",
        "R² = 0: No fit\n",
        "Example: R² = 0.85 → 85% of the variation in Y is explained by X.\n",
        "\n",
        "8. What is Multiple Linear Regression?\n",
        "\n",
        "- Multiple Linear Regression is a statistical method to model the relationship between one dependent variable and two or more independent variables.\n",
        "\n",
        "Equation:\n",
        "\n",
        "Y\n",
        "=\n",
        "β\n",
        "0\n",
        "+\n",
        "β\n",
        "1\n",
        "X\n",
        "1\n",
        "+\n",
        "β\n",
        "2\n",
        "X\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "β\n",
        "n\n",
        "X\n",
        "n\n",
        "+\n",
        "ε\n",
        "Y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        "​\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "​\n",
        " +ε\n",
        "It extends simple linear regression to handle multiple predictors.\n",
        "\n",
        "9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "\n",
        "- Main Difference:\n",
        "\n",
        "Simple Linear Regression uses one independent variable.\n",
        "Multiple Linear Regression uses two or more independent variables.\n",
        "\n",
        "10. What are the key assumptions of Multiple Linear Regression?\n",
        "\n",
        "- Key Assumptions of Multiple Linear Regression:\n",
        "\n",
        "Linearity: Linear relationship between predictors and outcome.\n",
        "Independence: Observations are independent.\n",
        "Homoscedasticity: Constant variance of errors.\n",
        "Normality: Errors are normally distributed.\n",
        "No Multicollinearity: Predictors are not highly correlated with each other.\n",
        "\n",
        "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "\n",
        "- Heteroscedasticity means the variance of errors is not constant across all levels of the independent variables.\n",
        "\n",
        "Effect:\n",
        "\n",
        "It violates regression assumptions.\n",
        "Leads to inefficient estimates and biased standard errors, affecting confidence intervals and p-values.\n",
        "\n",
        "12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "\n",
        "- To improve a Multiple Linear Regression model with high multicollinearity:\n",
        "\n",
        "Remove highly correlated predictors\n",
        "Use Principal Component Analysis (PCA)\n",
        "Apply Ridge or Lasso Regression\n",
        "Combine correlated variables\n",
        "Check Variance Inflation Factor (VIF) and drop variables with high VIF\n",
        "\n",
        "13. What are some common techniques for transforming categorical variables for use in regression models?\n",
        "\n",
        "- Common techniques for transforming categorical variables:\n",
        "\n",
        "One-Hot Encoding – Creates binary columns for each category\n",
        "Label Encoding – Assigns numeric labels to categories (for ordinal data)\n",
        "Ordinal Encoding – Maps categories to ordered integers\n",
        "Binary Encoding – Converts categories to binary numbers\n",
        "Target Encoding – Replaces categories with mean of target variable (use with caution)\n",
        "\n",
        "14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "\n",
        "- Interaction terms in Multiple Linear Regression capture the combined effect of two or more variables on the dependent variable.\n",
        "\n",
        "Role:\n",
        "They help model situations where the effect of one predictor depends on another.\n",
        "Example:\n",
        "Y\n",
        "=\n",
        "β\n",
        "0\n",
        "+\n",
        "β\n",
        "1\n",
        "X\n",
        "1\n",
        "+\n",
        "β\n",
        "2\n",
        "X\n",
        "2\n",
        "+\n",
        "β\n",
        "3\n",
        "(\n",
        "X\n",
        "1\n",
        "×\n",
        "X\n",
        "2\n",
        ")\n",
        "Y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        "​\n",
        " +β\n",
        "3\n",
        "​\n",
        " (X\n",
        "1\n",
        "​\n",
        " ×X\n",
        "2\n",
        "​\n",
        " )\n",
        "\n",
        "15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "\n",
        "- In Simple Linear Regression:\n",
        "The intercept is the expected value of Y when X = 0.\n",
        "\n",
        "In Multiple Linear Regression:\n",
        "The intercept is the expected value of Y when all independent variables = 0.\n",
        "\n",
        "It may be less interpretable if 0 is not meaningful for all predictors.\n",
        "\n",
        "16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "\n",
        "- The slope indicates the change in the dependent variable (Y) for a one-unit increase in an independent variable (X), holding other variables constant.\n",
        "\n",
        "Significance:\n",
        "\n",
        "Positive slope → Y increases as X increases\n",
        "Negative slope → Y decreases as X increases\n",
        "It directly affects the direction and magnitude of predictions.\n",
        "\n",
        "17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "\n",
        "- The intercept provides the baseline value of the dependent variable (Y) when all independent variables are zero.\n",
        "\n",
        "Context:\n",
        "\n",
        "It anchors the regression line.\n",
        "Helps understand the starting point of Y before any effect of predictors.\n",
        "May or may not have real-world meaning, depending on the data.\n",
        "\n",
        "18. What are the limitations of using R² as a sole measure of model performance?\n",
        "\n",
        "- Limitations of R² as a sole measure:\n",
        "\n",
        "Doesn't indicate model accuracy – High R² doesn't mean good predictions.\n",
        "Ignores overfitting – R² increases with more variables, even if they’re irrelevant.\n",
        "Doesn't reflect model bias or error – Says nothing about residual patterns.\n",
        "Not useful for comparing models with different target variables.\n",
        "Can be misleading in non-linear relationships.\n",
        "\n",
        "19. How would you interpret a large standard error for a regression coefficient?\n",
        "\n",
        "- A large standard error for a regression coefficient suggests:\n",
        "\n",
        "High uncertainty in the estimate\n",
        "The coefficient may not be significantly different from zero\n",
        "The variable might have a weak or no effect on the dependent variable\n",
        "Could indicate multicollinearity or insufficient data\n",
        "\n",
        "20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "\n",
        "- Identification in Residual Plots:\n",
        "\n",
        "Heteroscedasticity appears as a funnel shape or pattern in the plot of residuals vs. predicted values.\n",
        "Residuals spread increase or decrease with fitted values, rather than being randomly scattered.\n",
        "Why it’s important to address:\n",
        "\n",
        "It violates regression assumptions\n",
        "Leads to biased standard errors, affecting p-values and confidence intervals\n",
        "Can result in misleading conclusions about predictor significance\n",
        "\n",
        "21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "\n",
        "- If a model has high R² but low adjusted R², it means:\n",
        "\n",
        "The model includes irrelevant or redundant predictors\n",
        "R² increased just by adding variables, but they don’t improve the model\n",
        "Adjusted R² penalizes unnecessary predictors, reflecting the true explanatory power\n",
        "Conclusion: The model may be overfitted.\n",
        "\n",
        "22. Why is it important to scale variables in Multiple Linear Regression?\n",
        "\n",
        "- Importance of Scaling in Multiple Linear Regression:\n",
        "\n",
        "Ensures equal influence of variables with different units/ranges\n",
        "Improves numerical stability of the model\n",
        "Helps interpret coefficients (especially in regularized models like Ridge/Lasso)\n",
        "Prevents dominance of large-scale features over smaller ones\n",
        "\n",
        "23. What is polynomial regression?\n",
        "\n",
        "- Polynomial Regression is a form of regression where the relationship between the independent variable\n",
        "X\n",
        "X and the dependent variable\n",
        "Y\n",
        "Y is modeled as an nth-degree polynomial.\n",
        "\n",
        "Equation:\n",
        "\n",
        "Y\n",
        "=\n",
        "β\n",
        "0\n",
        "+\n",
        "β\n",
        "1\n",
        "X\n",
        "+\n",
        "β\n",
        "2\n",
        "X\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "β\n",
        "n\n",
        "X\n",
        "n\n",
        "+\n",
        "ε\n",
        "Y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X+β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        " +ε\n",
        "It captures non-linear relationships while still using a linear model in terms of coefficients.\n",
        "\n",
        "24. How does polynomial regression differ from linear regression?\n",
        "\n",
        "- Polynomial Regression vs. Linear Regression:\n",
        "\n",
        "Linear Regression models a straight-line relationship:\n",
        "Y\n",
        "=\n",
        "β\n",
        "0\n",
        "+\n",
        "β\n",
        "1\n",
        "X\n",
        "Y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X\n",
        "Polynomial Regression models a curved (non-linear) relationship using higher-degree terms:\n",
        "Y\n",
        "=\n",
        "β\n",
        "0\n",
        "+\n",
        "β\n",
        "1\n",
        "X\n",
        "+\n",
        "β\n",
        "2\n",
        "X\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "β\n",
        "n\n",
        "X\n",
        "n\n",
        "Y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X+β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "\n",
        "Key Difference:\n",
        "Polynomial regression can fit non-linear data, while linear regression cannot.\n",
        "\n",
        "25. When is polynomial regression used?\n",
        "\n",
        "- Polynomial regression is used when:\n",
        "\n",
        "The relationship between variables is non-linear.\n",
        "A linear model underfits the data.\n",
        "You want to model curves or complex trends in data.\n",
        "The residual plot of linear regression shows a pattern (not random).\n",
        "\n",
        "26. What is the general equation for polynomial regression?\n",
        "\n",
        "- The general equation for polynomial regression is:\n",
        "\n",
        "Y\n",
        "=\n",
        "β\n",
        "0\n",
        "+\n",
        "β\n",
        "1\n",
        "X\n",
        "+\n",
        "β\n",
        "2\n",
        "X\n",
        "2\n",
        "+\n",
        "β\n",
        "3\n",
        "X\n",
        "3\n",
        "+\n",
        "⋯\n",
        "+\n",
        "β\n",
        "n\n",
        "X\n",
        "n\n",
        "+\n",
        "ε\n",
        "Y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X+β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        " +β\n",
        "3\n",
        "​\n",
        " X\n",
        "3\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        " +ε\n",
        "Where:\n",
        "\n",
        "Y\n",
        "Y: Dependent variable\n",
        "X\n",
        "X: Independent variable\n",
        "β\n",
        "0\n",
        ",\n",
        "β\n",
        "1\n",
        ",\n",
        "…\n",
        ",\n",
        "β\n",
        "n\n",
        "β\n",
        "0\n",
        "​\n",
        " ,β\n",
        "1\n",
        "​\n",
        " ,…,β\n",
        "n\n",
        "​\n",
        " : Coefficients\n",
        "ε\n",
        "ε: Error term\n",
        "n\n",
        "n: Degree of the polynomial\n",
        "\n",
        "27. Can polynomial regression be applied to multiple variables?\n",
        "\n",
        "- Yes, polynomial regression can be applied to multiple variables.\n",
        "\n",
        "This is called multivariate polynomial regression, where polynomial terms include:\n",
        "\n",
        "Powers of individual variables (e.g.,\n",
        "X\n",
        "1\n",
        "2\n",
        ",\n",
        "X\n",
        "2\n",
        "3\n",
        "X\n",
        "1\n",
        "2\n",
        "​\n",
        " ,X\n",
        "2\n",
        "3\n",
        "​\n",
        " )\n",
        "Interaction terms (e.g.,\n",
        "X\n",
        "1\n",
        "×\n",
        "X\n",
        "2\n",
        "X\n",
        "1\n",
        "​\n",
        " ×X\n",
        "2\n",
        "​\n",
        " )\n",
        "It captures non-linear relationships among multiple predictors.\n",
        "\n",
        "28. What are the limitations of polynomial regression?\n",
        "\n",
        "- Limitations of Polynomial Regression:\n",
        "\n",
        "Overfitting – High-degree polynomials may fit noise, not the trend.\n",
        "Poor extrapolation – Predictions outside data range can be unreliable.\n",
        "Complexity – More terms make the model harder to interpret.\n",
        "Collinearity – Polynomial terms can be highly correlated.\n",
        "Computational cost – Increases with degree and number of variables.\n",
        "\n",
        "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "\n",
        "- Methods to evaluate model fit when selecting polynomial degree:\n",
        "\n",
        "Cross-validation – Tests model performance on unseen data.\n",
        "Adjusted R² – Rewards goodness of fit but penalizes complexity.\n",
        "AIC/BIC (Akaike/Bayesian Information Criterion) – Penalize overfitting.\n",
        "Residual plots – Check for patterns or randomness in errors.\n",
        "Mean Squared Error (MSE)/RMSE – Measures prediction error.\n",
        "\n",
        "30. Why is visualization important in polynomial regression?\n",
        "\n",
        "- Visualization is important in polynomial regression because it:\n",
        "\n",
        "Reveals the curve fitted to the data\n",
        "Helps detect overfitting or underfitting\n",
        "Shows how well the model captures trends\n",
        "Identifies outliers or unusual patterns\n",
        "Improves interpretability of complex relationships\n",
        "\n",
        "31. How is polynomial regression implemented in Python?\n",
        "\n",
        "- Polynomial regression in Python is typically implemented using scikit-learn as follows:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uHhpNSQfHrED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Sample data\n",
        "X = [[1], [2], [3], [4]]\n",
        "y = [2, 6, 14, 28]\n",
        "\n",
        "# Create a polynomial regression model (degree 2)\n",
        "model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X)"
      ],
      "metadata": {
        "id": "KV5t92tcLBHT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}